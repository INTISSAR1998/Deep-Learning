{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transfer Learning V2 .ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"_Ty-muTmGsPk"},"source":["# **Transfer Learning in Keras with Computer Vision Models**"]},{"cell_type":"markdown","metadata":{"id":"hwQnWjXGGvWh"},"source":["- La phase d'apprentissage des modèles de réseaux de neurones convolutionnels profonds peuvent prendre des jours, voire des semaines, sur de très grands ensembles de données.\n","\n","- Un moyen de raccourcir, consiste à réutiliser les poids(Weights) de modèle à partir de modèles pré-entrainés qui ont été développés pour des benchmark (datasets) standards de vision par ordinateur, tels que les tâches de reconnaissance d'image ImageNet.\n","\n","- Les modèles les plus performants peuvent être téléchargés et utilisés directement, ou intégrés dans un nouveau modèle pour vos propres problèmes de vision par ordinateur.\n","\n","- Keras offre un accès pratique à de nombreux modèles les plus performants sur les tâches de reconnaissance d'image ImageNet telles que **VGG**, **Inception** et **ResNet**."]},{"cell_type":"markdown","metadata":{"id":"Q0YqQH0xINXe"},"source":["## Transfer Learning for Image Recognition\n","\n","Transfert d'apprentissage pour la reconnaissance d'image"]},{"cell_type":"markdown","metadata":{"id":"P1J8A3ylIkW9"},"source":["- Une gamme de modèles hautement performants a été développée pour la classification des images et démontrée lors du défi annuel de reconnaissance visuelle à grande échelle **ImageNet** (Large Scale Visual Recognition Challenge), or **ILSVRC**.\n","\n","\n","Cela est souhaitable pour plusieurs raisons, notamment:\n","\n","- Fonctionnalités apprises utiles: Les modèles ont appris à détecter des caractéristiques génériques à partir de photographies, étant donné qu'ils ont été formés sur plus de 1 000 000 d'images pour 1 000 catégories.\n","\n","- Performances de pointe: les modèles ont atteint des performances de pointe et restent efficaces sur la tâche de reconnaissance d'image spécifique pour laquelle ils ont été développés.\n","\n","- Facilement accessible: les poids des modèles sont fournis sous forme de fichiers téléchargeables gratuitement et de nombreuses bibliothèques fournissent des API pratiques pour télécharger et utiliser directement les modèles, y compris Keras.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"e17nKoCnIS_z"},"source":["## Comment utiliser des modèles pré-entrainés \n","How to Use Pre-Trained Models"]},{"cell_type":"markdown","metadata":{"id":"9nAi8XL1MYk1"},"source":["Nous pouvons résumer certaines utilisation de ces modèles comme suit:\n","\n","- **Classifier**: Le modèle pré-entrainé est utilisé directement pour classer les nouvelles images.\n","- **Standalone Feature Extractor**:  Le modèle pré-entrainé, ou une partie du modèle, est utilisé pour prétraiter les images et extraire les fonctionnalités pertinentes.\n"," \n","- **Integrated Feature Extractor**: \n","\n","Le modèle pré-entrainé, ou une partie du modèle, est intégré dans un nouveau modèle, mais les couches du modèle pré-entrainé sont figées pendant la formation.\n","\n","- **Weight Initialization**:  Le modèle préentrainé, ou une partie du modèle, est intégré dans un nouveau modèle, et les couches du modèle préformé sont entraînées conjointement avec le nouveau modèle\n","\n","Il peut ne pas être clair quelle utilisation du modèle préformé peut donner les meilleurs résultats sur votre nouvelle tâche de vision par ordinateur, donc une certaine expérimentation peut être nécessaire.\n"]},{"cell_type":"markdown","metadata":{"id":"D938tP-gIbm_"},"source":["## Modèles pour un apprentissage par transfert\n","Models for Transfer Learning"]},{"cell_type":"markdown","metadata":{"id":"ysF7CzQ4NytY"},"source":["Models for image recognition that can be downloaded and used : \n","\n","Perhaps three of the more popular models are as follows:\n","\n","- VGG (e.g. VGG16 or VGG19).\n","- GoogLeNet (e.g. InceptionV3).\n","- Residual Network (e.g. ResNet50).\n","\n","\n","Keras provides access to a number of top-performing pre-trained models that were developed for image recognition tasks.They are available via the Applications API, and include functions to load a model with or without the pre-trained weights. \n"]},{"cell_type":"markdown","metadata":{"id":"bfBe07mAf44g"},"source":["## **VGG-16**"]},{"cell_type":"markdown","metadata":{"id":"JxyTSmSHEgKL"},"source":["Le modèle VGG16 a été développé par le Visual Graphics Group (VGG) à Oxford et a été décrit dans l'article de 2014 intitulé\n","“[Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556).”"]},{"cell_type":"markdown","metadata":{"id":"upYwrxSKa-_k"},"source":["### **Architecture de VGG-16**"]},{"cell_type":"markdown","metadata":{"id":"Ys37wfqratj_"},"source":["L'objet de notre étude est VGG-16, une version du réseau de neurones convolutif très connu appelé VGG-Net. Nous allons d'abord l'implémenter de A à Z pour découvrir Keras, puis nous allons voir comment classifier des images de manière efficace. Pour cela, nous allons exploiter le réseau VGG-16 pré-entraîné fourni par Keras, et mettre en oeuvre le Transfer Learning. "]},{"cell_type":"markdown","metadata":{"id":"5v-5pAyMbLLw"},"source":["VGG-16 est constitué de plusieurs couches, dont : \n","- 13 couches de convolution et \n","- 3 fully-connected. \n","\n","Il doit donc apprendre les poids de 16 couches.\n","\n","Il prend en entrée une image en couleurs de taille 224  × 224 px et la classifie dans une des 1000 classes. Il renvoie donc un vecteur de taille 1000, qui contient les probabilités d'appartenance à chacune des classes. \n","\n","L'architecture de VGG-16 est illustrée par les schémas ci-dessous :"]},{"cell_type":"markdown","metadata":{"id":"HlrZz2QabYna"},"source":["<figure>\n","<center>\n","<img src='https://docs.google.com/uc?export=download&id=1FC7wtJqMO6SnTesuxV7R_gceWPE0OIdf'/>\n","    \n","    \n","<figcaption> Architecture de VGG-16\n","\n","</figcaption></center>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"F0tUecTfcEay"},"source":["<figure>\n","<center>\n","<img src='https://docs.google.com/uc?export=download&id=11gD2ygfeRAA-u66zaEyPYtN0_XYxjK7L'/>\n","    \n","    \n","<figcaption>  Représentation 3D de l'architecture de VGG-16\n","\n","</figcaption></center>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"DLsgJVw5cTXd"},"source":["- Chaque couche de convolution utilise des filtres en couleurs de taille 3  ×3 px, déplacés avec un pas de 1 pixel. Le zero-padding vaut 1 pixel afin que les volumes en entrée aient les mêmes dimensions en sortie. Le nombre de filtres varie selon le \"bloc\" dans lequel la couche se trouve. De plus, un paramètre de biais est introduit dans le produit de convolution pour chaque filtre.\n","\n","- Chaque couche de convolution a pour fonction d'activation une ReLU. Autrement dit, il y a toujours une couche de correction ReLU après une couche de convolution.\n","\n","- L'opération de pooling est réalisée avec des cellules de taille 2 × 2 px et un pas de 2 px – les cellules ne se chevauchent donc pas.\n","\n","- Les deux premières couches fully-connected calculent chacune un vecteur de taille 4096, et sont chacune suivies d'une couche ReLU. La dernière renvoie le vecteur de probabilités de taille 1000 (le nombre de classes) en appliquant la fonction softmax. De plus, ces trois couches utilisent un paramètre de biais pour chaque élément du vecteur en sortie. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"jhZXAw14cwGH"},"source":["Retrouvons les dimensions du volume renvoyé par chaque couche. \n","\n","- La première couche de convolution a pour paramètres $K=64, F_C=3,$ $ S_C=1 etP=1.$  \n","Elle renvoie donc un volume de dimensions $W_C×H_C×D_C $avec $W_C=H_C= \\frac{224−3+2}{1}+1=224 $ et $ DC=64$. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"o_XmnDfkuwjx"},"source":["<figure>\n","<center>\n","<img src='https://docs.google.com/uc?export=download&id=1reD0W2Q-zarqvGm9mR76upjpUNXALmJk' />\n","    \n","<figcaption> \n","\n","</figcaption></center>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"69Ta8wI_uyvJ"},"source":["<figure>\n","<center>\n","<img src='https://docs.google.com/uc?export=download&id=1HYoeY3Wpl1lGvq56tSatZ4PEmza9PG80' />\n","<figcaption> \n","\n","</figcaption></center>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"nRLHzpV3vpbx"},"source":["<figure>\n","<center>\n","<img src='https://docs.google.com/uc?export=download&id=1E3FlsP2Zqgq6CGZZ_zyUPm1GWNnWGy1O' />\n","    \n","<figcaption> \n","\n","</figcaption></center>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"Y4G2X4eGwDdD"},"source":["- La couche de ReLU ne modifie pas les dimensions du volume en entrée. On applique le même raisonnement pour les couches de convolution et ReLU suivantes.\n","\n","- La couche de pooling du premier bloc reçoit donc en entrée un volume de dimensions $224×224×64$. Comme elle a pour paramètres $F_P=2$ et $S_P=2$, elle renvoie un volume de dimensions $W_P×H_P×D_P$ avec  $W_P=H_C=\\frac{224−2}{2}+1=112$ et $D_P=64$ . \n","- Finalement, le volume en sortie du premier bloc est de dimensions $112×112×64$. "]},{"cell_type":"markdown","metadata":{"id":"3PGJB7O2wGlU"},"source":["<figure>\n","<center>\n","<img src='https://docs.google.com/uc?export=download&id=1M35822M9GiydZIIMUJjrXI3svMdgt2WB' />\n","    \n","<figcaption> \n","\n","</figcaption></center>\n","</figure>\n"]},{"cell_type":"markdown","metadata":{"id":"lp0uXlkxe8Ae"},"source":["**Combien de paramètres VGG-16 doit-il apprendre pendant la phase d'entraînement ?**"]},{"cell_type":"markdown","metadata":{"id":"aOwIm-p8fFLd"},"source":["- VGG-16 apprend 138 357 544 paramètres ! Pour trouver ce petit nombre, il suffit de compter les poids de toutes les couches de convolution et fully-connected, sans oublier les paramètres de biais.\n","\n","- Par exemple pour la première couche de convolution, le réseau doit apprendre 64 filtres en couleurs (donc de profondeur 3) de taille 3 × 3, ainsi qu'un paramètre de biais pour chaque filtre. Cela fait un total de $(3*3*3)*64 + 64 = 1792$ paramètres.\n","\n","- Le nombre de paramètres d'une couche fully-connected s'obtient en multipliant le nombre d'éléments en sortie avec celui en entrée, et en ajoutant le nombre de biais. Ainsi, le réseau doit apprendre $7*7*512*4096 + 4096 = 102 764 544$ paramètres pour la première couche fully-connected, $4096*4096 + 4096 = 16 781 312$ pour la deuxième, et $4096*1000 + 1000 = 4 097 000$ pour la dernière."]},{"cell_type":"markdown","metadata":{"id":"2Byf6b6DfpHH"},"source":["### **Implémentation de VGG-16**"]},{"cell_type":"markdown","metadata":{"id":"cKrJKtcDgJ5I"},"source":["Maintenant que vous maîtrisez l'architecture de VGG-16, nous pouvons passer à la partie la plus rigolote : l'implémentation !\n","\n","Implémenter un réseau de neurones avec Keras revient à créer un modèle  Sequential  et à l'enrichir avec les couches correspondantes dans le bon ordre. L'étape la plus difficile est de définir correctement les paramètres de chacune des couches – d'où l'importance de bien comprendre l'architecture du réseau !\n","\n","La création du modèle se fait comme ci-dessous :"]},{"cell_type":"code","metadata":{"id":"FG9jIPrLax36"},"source":["from keras.models import Sequential\n","\n","my_VGG16 = Sequential()  # Création d'un réseau de neurones vide "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3uROGFlVgXIw"},"source":["- Les couches s'ajoutent soit en tant que paramètres d'entrée du constructeur  **Sequential()** , soit une par une avec la méthode mon_reseau_VGG.add() .\n","\n","- Les couches de **convolution**, **pooling** et **fully-connected** correspondent à des instances des classes respectives **Conv2D**, **MaxPooling2D** et **Dense** du module  keras.layers .\n","- Une couche **ReLU** peut être créée soit en instanciant la classe Activation, soit en ajoutant un argument au constructeur de la couche qui la précède."]},{"cell_type":"markdown","metadata":{"id":"shsgbW6Fg_Vw"},"source":["Pour construire une couche de convolution, nous devons préciser le nombre de filtres utilisés, leur taille, le pas et le zero-padding. Ils correspondent respectivement aux arguments  : \n","- filters ,  \n","- kernel_size ,  \n","- strides  et  \n","- padding  \n","\n","du constructeur de la classe  Conv2D ."]},{"cell_type":"markdown","metadata":{"id":"z3oCxntThQ4P"},"source":["Seulement deux valeurs sont possibles pour  **padding**  : '**same**' ou '**valid**'. La couche de convolution réduit la taille du volume en entrée avec l'option 'valid', mais la conserve avec 'same'. Pour **VGG-16**, on utilisera donc toujours l'option 'same'."]},{"cell_type":"markdown","metadata":{"id":"LIq1ekA7hkya"},"source":["S'il s'agit de la toute première couche de convolution, il faut préciser dans l'argument **input_shape**  les dimensions des images en entrée du réseau. Pour VGG-16, **input_shape = (224, 224, 3)** .\n","\n","Enfin, pour indiquer la présence d'une couche ReLU juste après la couche de convolution, on ajoute l'argument **activation** = '**relu**' .\n","\n","Une couche de pooling est définie par la taille des cellules de pooling et le pas avec lequel on les déplace. Ces paramètres sont précisés dans les arguments respectifs pool_size  et strides  du constructeur de la classe MaxPooling2D . \n","\n","La construction du premier bloc de couches est détaillée ci-dessous :"]},{"cell_type":"code","metadata":{"id":"8z1u6YDDgRhs"},"source":["from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPooling2D\n","\n","my_VGG16 = Sequential()  # Création d'un réseau de neurones vide \n","\n","# Ajout de la première couche de convolution, suivie d'une couche ReLU\n","my_VGG16.add(Conv2D(64, (3, 3), input_shape=(224, 224, 3), padding='same', activation='relu'))\n","\n","# Ajout de la deuxième couche de convolution, suivie  d'une couche ReLU\n","my_VGG16.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n","\n","# Ajout de la première couche de pooling\n","my_VGG16.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MG1xuviWiQn-"},"source":["il ne reste plus qu'à ajouter les couches fully-connected, en créant des objets de la classe  **Dense**. \n","- Ce type de couche reçoit en entrée un vecteur 1D. Il faut donc convertir les matrices 3D renvoyées par la dernière couche de pooling. \n","- Pour cela, on instancie la classe Flatten juste avant la première couche fully-connected.\n","\n","- L'argument  units  du constructeur de  Dense  permet de préciser la taille du vecteur en sortie. De plus, si une correction ReLU ou softmax est effectuée juste après la couche fully-connected, on l'indique dans le paramètre  activation\n","\n","- Ainsi, les trois dernières couches fully-connected et leur fonction d'activation (ReLU pour les deux premières, softmax pour la dernière) sont ajoutées de la manière suivante :"]},{"cell_type":"code","metadata":{"id":"lvQfYqemiMjT"},"source":["from keras.layers import Flatten, Dense\n","\n","my_VGG16.add(Flatten())  # Conversion des matrices 3D en vecteur 1D\n","\n","# Ajout de la première couche fully-connected, suivie d'une couche ReLU\n","my_VGG16.add(Dense(4096, activation='relu'))\n","\n","# Ajout de la deuxième couche fully-connected, suivie d'une couche ReLU\n","my_VGG16.add(Dense(4096, activation='relu'))\n","\n","# Ajout de la dernière couche fully-connected qui permet de classifier\n","my_VGG16.add(Dense(1000, activation='softmax'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gEIrh1f1i5X4"},"source":["Une fois le modele est implémenté, Nous devons : \n","- **Compiler** notre modèle avec la méthode **Sequential.compile()**, \n","- l'**entraîner** avec **Sequential.fit()**. \n","- Mais la phase d'entraînement peut prendre au moins un mois, même avec un GPU ! Nous allons donc voir comment gagner du temps dans les prochaines sections. "]},{"cell_type":"markdown","metadata":{"id":"S9BG6ipNj2UZ"},"source":["### **Utilisation du VGG-16 pré-entraîné**"]},{"cell_type":"markdown","metadata":{"id":"dXBjOiBlkM1m"},"source":["Vous savez maintenant comment implémenter un réseau de neurones convolutif de A à Z avec Keras. Dans cette partie, nous allons apprendre à classifier des images avec le modèle VGG-16 fourni par Keras et pré-entraîné sur ImageNet.\n","\n","La première étape consiste à charger ce modèle avec la classe VGG16 de  keras.applications.vgg16 :"]},{"cell_type":"code","metadata":{"id":"xnCn2nediwbS"},"source":["from keras.applications.vgg16 import VGG16\n","model = VGG16() # Création du modèle VGG-16 implementé par Keras"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-WSjkumHkfCM"},"source":["Par défaut, le constructeur  **VGG16()**  crée le réseau VGG-16 pré-entraîné sur **ImageNet**. Si à l'avenir, pour d'autres projets, vous souhaitez initialiser aléatoirement les poids, il faudra préciser  **weight=None**  en argument.\n","\n","Le constructeur possède d'autres paramètres pour faire du Transfer Learning, que nous allons utiliser dans la partie suivante."]},{"cell_type":"markdown","metadata":{"id":"BEPOE0s7kp0G"},"source":["Nous allons utiliser ce réseau pré-entraîné pour classer une image dans une des **1000** catégories d'**ImageNet**.\n","\n","Nous devons d'abord charger l'image et la pré-traiter afin qu'elle respecte bien les spécifications des images en entrée de VGG-16. Pour cela, nous allons utiliser les fonctions du module  **keras.preprocessing.image**  et  **keras.preprocessing.vgg16**  :\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_NmEu47WlGNW"},"source":["1. VGG-16 reçoit des images de taille (224, 224, 3) : la fonction  **load_img**  permet de charger l'image et de la redimensionner correctement \n","\n","2. Keras traite les images comme des tableaux numpy :  **img_to_array**  permet de convertir l'image chargée en tableau numpy\n","\n","3. Le réseau doit recevoir en entrée une collection d'images, stockée dans un tableau de 4 dimensions, où les dimensions correspondent (dans l'ordre) à (nombre d'images, largeur, hauteur, profondeur). Pour l'instant, nous donnons qu'une image en entrée : numpy.reshape  permet d'ajouter la première dimension (nombre d'images = 1) à notre image.\n","\n","4. Enfin,  **preprocess_input**  permet d'appliquer les mêmes pré-traitements que ceux utilisés sur l'ensemble d'apprentissage lors du pré-entraînement. \n","\n","Ainsi, on prépare l'image comme ci-dessous :"]},{"cell_type":"code","metadata":{"id":"SVFU_pT1kXOV"},"source":["from keras.preprocessing.image import load_img, img_to_array\n","from keras.applications.vgg16 import preprocess_input\n","\n","img = load_img('dog.jpeg', target_size=(224, 224))  # Charger l'image\n","img = img_to_array(img)  # Convertir en tableau numpy\n","img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))  # Créer la collection d'images (un seul échantillon)\n","img = preprocess_input(img)  # Prétraiter l'image comme le veut VGG-16\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9MzXlghOlabw"},"source":["Nous pouvons maintenant donner l'image en entrée du réseau et prédire sa classe :"]},{"cell_type":"code","metadata":{"id":"7a7tQnOzldoW"},"source":["y = model.predict(img)  # Prédir la classe de l'image (parmi les 1000 classes d'ImageNet)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uzwkzHGjllsv"},"source":["On obtient la sortie finale du réseau, c'est-à-dire une liste de 1000 probabilités.\n","\n","Les classes correspondant à ces probabilités ne sont pas explicitement données. La fonction  decode_predictions  de  keras.applications.vgg16  permet alors de récupérer cette information.  Ainsi, on peut faire un top 3 des classes les plus probables de l'image : "]},{"cell_type":"code","metadata":{"id":"rQBmiTqFlrVP","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1576669742916,"user_tz":-60,"elapsed":585,"user":{"displayName":"Jaafar Chaaouri","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mA4-opawK4Z_wqlRSPkY1Af9MKQW9Pm0DwTKwnPCQ=s64","userId":"16971688257570499832"}},"outputId":"bfaa7cd5-81b9-4123-e4ea-4659c066b439"},"source":["from keras.applications.vgg16 import decode_predictions\n","# Afficher les 3 classes les plus probables\n","print('Top 3 :', decode_predictions(y, top=3)[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Top 3 : [('n02099712', 'Labrador_retriever', 0.6361298), ('n02099601', 'golden_retriever', 0.33132473), ('n02104029', 'kuvasz', 0.00655517)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uvW-0QJsmI87"},"source":["### **Transfer Learning**"]},{"cell_type":"markdown","metadata":{"id":"rrDYaI5cmLo1"},"source":["Dans la section précédente, nous avons utilisé le réseau VGG-16 fourni par Keras pour résoudre le même problème de classification que celui sur lequel il a été pré-entraîné (classification à 1000 classes avec ImageNet). En pratique, vous serez très probablement confrontés à un nouveau problème de classification. Dans ce cas, savoir mettre en oeuvre le Transfer Learning vous sera très utile ! \n","\n","Tout d'abord, il faut bien comprendre les stratégies possibles,  \n","- **Fine-tuning total**, \n","- **Extraction des features**, et \n","- **Fine-tuning partiel**. \n","\n","Dans les trois cas, il faut remplacer les dernières couches **fully-connected** qui permettent de classifier l'image dans une des 1000 classes ImageNet) par un classifieur plus adapté à notre problème.  \n","\n","Par exemple, supposons qu'on veuille différencier un chat d'un chien (classification binaire). La suppression des dernières couches se fait en ajoutant l'argument  **include_top = False** lors de l'import du modèle pré-entraîné. Dans ce cas, il faut aussi préciser les dimensions des images en entrée (**input_shape** ) : "]},{"cell_type":"code","metadata":{"id":"BhhzOzgfKfgK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fxbweQ4-nXHy"},"source":["from keras.applications.vgg16 import VGG16\n","from keras.layers import Dense\n","\n","# Charger VGG-16 pré-entraîné sur ImageNet et sans les couches fully-connected\n","model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n","\n","# Récupérer la sortie de ce réseau\n","x = model.output\n","\n","# Ajouter la nouvelle couche fully-connected pour la classification à 10 classes\n","predictions = Dense(10, activation='softmax')(x)\n","\n","# Définir le nouveau modèle\n","new_model = Model(inputs=model.input, outputs=predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"80y62JA8nUXl"},"source":["### **Stratégie #1 : fine-tuning total**\n"]},{"cell_type":"markdown","metadata":{"id":"YTKBXsyrni-n"},"source":["Ici, on entraîne tout le réseau, donc il faut rendre toutes les couches \"entraînables\" :\n","\n"]},{"cell_type":"code","metadata":{"id":"x2_AJPQlmKtv"},"source":["for layer in model.layers:\n","   layer.trainable = True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"celhti1Lnwjo"},"source":["### **Stratégie #2 : extraction de features**"]},{"cell_type":"markdown","metadata":{"id":"FErtZea3n63A"},"source":["On entraîne seulement le nouveau classifieur et on ne ré-entraîne pas les autres couches :"]},{"cell_type":"code","metadata":{"id":"lqInpqjinz-0"},"source":["for layer in model.layers:\n","   layer.trainable = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Xnq_SI3n-Zq"},"source":["### **Stratégie #3 : fine-tuning partiel**"]},{"cell_type":"markdown","metadata":{"id":"1EZ821sNoLUn"},"source":["On entraîne le nouveau classifieur et les couches hautes :"]},{"cell_type":"code","metadata":{"id":"WPro0fPCny4_"},"source":["# Ne pas entraîner les 5 premières couches (les plus basses) \n","for layer in model.layers[:5]:\n","   layer.trainable = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HQG6CyCpoRyQ"},"source":["### **Entraînement du réseau**"]},{"cell_type":"markdown","metadata":{"id":"LzQUb--8oXir"},"source":["Il ne reste plus qu'à compiler le nouveau modèle, puis à l'entraîner  :"]},{"cell_type":"code","metadata":{"id":"ad2PD8GkoUS7"},"source":["# Compiler le modèle \n","new_model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])\n","\n","# Entraîner sur les données d'entraînement (X_train, y_train)\n","model_info = new_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=2)\n"],"execution_count":null,"outputs":[]}]}